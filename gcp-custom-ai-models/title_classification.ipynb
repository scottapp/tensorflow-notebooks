{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9853c90-71d3-40b2-bcbd-145373c406c6",
   "metadata": {},
   "source": [
    "# Hacker News Title Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24321ee-da3e-4b5c-b41d-5390f463d134",
   "metadata": {},
   "source": [
    "The original blog is on google\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/ai-in-depth-creating-preprocessing-model-serving-affinity-with-custom-online-prediction-on-ai-platform-serving\n",
    "\n",
    "* We are going to train and build a title text classification model using kera and the sequence model method. \n",
    "* We will then embed the trained model in a fastapi prediction server so that we can send classification jobs using plain old http requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c8b94a-7fdb-4f2b-a9c2-cb6fd36f17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset to local directory using gsutil\n",
    "#!gsutil cp -r gs://cloud-training-demos/blogs/CMLE_custom_prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46532bb-5db6-4115-b90c-9449b6aa79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc2dfac-c19c-43d2-a5d5-73627326440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72161, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>github</th>\n",
       "      <th>feminist-software-foundation complains about reddit on a github pull request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70983</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>nintendo  37 million wii consoles sold in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>nsa leaks revive russian push for internet reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66705</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>lg to develop youtube phone - keeping up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40833</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>venezuela s inflated vision of beauty  video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28520</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>stubborn skills gap in america s work force</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           github  \\\n",
       "70983  techcrunch   \n",
       "39988     nytimes   \n",
       "66705  techcrunch   \n",
       "40833     nytimes   \n",
       "28520     nytimes   \n",
       "\n",
       "      feminist-software-foundation complains about reddit on a github pull request  \n",
       "70983  nintendo  37 million wii consoles sold in the ...                            \n",
       "39988  nsa leaks revive russian push for internet reg...                            \n",
       "66705  lg to develop youtube phone - keeping up with ...                            \n",
       "40833      venezuela s inflated vision of beauty  video                             \n",
       "28520        stubborn skills gap in america s work force                            "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "print(train.shape)\n",
    "train = shuffle(train, random_state=22)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c598a0-aeb0-467e-8eeb-51e730a59529",
   "metadata": {},
   "source": [
    "### Load train and eval data and one-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6be0a19-8d2f-4c5d-9507-41336e778fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: nintendo  37 million wii consoles sold in the u.s. to date\n",
      "label: [0. 0. 1.]\n",
      "72161 72161\n",
      "text: geoip module on nodejs now is a c   addon\n",
      "label: [1. 0. 0.]\n",
      "24040 24040\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_file, eval_file):\n",
    "    encoder = OneHotEncoder()\n",
    "    \n",
    "    train_set = pd.read_csv('data/{}'.format(train_file), sep='\\t')\n",
    "    train_set.columns = ['label', 'title']\n",
    "    train_set = shuffle(train_set, random_state=22)\n",
    "    train_hot = encoder.fit_transform(train_set[['label']]).toarray()\n",
    "    \n",
    "    eval_set = pd.read_csv('data/{}'.format(eval_file), sep='\\t')\n",
    "    eval_set.columns = ['label', 'title']\n",
    "    eval_hot = encoder.transform(eval_set[['label']]).toarray()\n",
    "    \n",
    "    return (train_set['title'].values, train_hot), (eval_set['title'].values, eval_hot)\n",
    "\n",
    "(train_texts, train_labels), (eval_texts, eval_labels) = load_data('train.tsv', 'eval.tsv')\n",
    "print('text: %s' % train_texts[0])\n",
    "print('label: %s' % train_labels[0])\n",
    "print(len(train_texts), len(train_labels))\n",
    "\n",
    "print('text: %s' % eval_texts[0])\n",
    "print('label: %s' % eval_labels[0])\n",
    "print(len(eval_texts), len(eval_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c413a976-6bb6-4dfa-9270-937b204eb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(object):\n",
    " def __init__(self, vocab_size, max_sequence_length):\n",
    "   self._vocab_size = vocab_size\n",
    "   self._max_sequence_length = max_sequence_length\n",
    "   self._tokenizer = None\n",
    "\n",
    " def fit(self, text_list):       \n",
    "   # Create vocabulary from input corpus.\n",
    "   tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "   tokenizer.fit_on_texts(text_list)\n",
    "   self._tokenizer = tokenizer\n",
    "\n",
    " def transform(self, text_list):       \n",
    "   # Transform text to sequence of integers\n",
    "   text_sequence = self._tokenizer.texts_to_sequences(text_list)\n",
    "\n",
    "   # Fix sequence length to max value. Sequences shorter than the length are\n",
    "   # padded in the beginning and sequences longer are truncated\n",
    "   # at the beginning.\n",
    "   padded_text_sequence = sequence.pad_sequences(text_sequence, maxlen=self._max_sequence_length)\n",
    "   return padded_text_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e87bf9-2e52-4658-99fc-9853a81e3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "((train_texts, train_labels), (eval_texts, eval_labels)) = load_data( 'train.tsv', 'eval.tsv')\n",
    "\n",
    "# Create vocabulary from training corpus.\n",
    "processor = TextPreprocessor(VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "processor.fit(train_texts)\n",
    "\n",
    "# Preprocess the data\n",
    "train_texts_vectorized = processor.transform(train_texts)\n",
    "eval_texts_vectorized = processor.transform(eval_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60387103-3c48-41ab-a3c6-2ee0682c2d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   34 3300  680 1727    4  543   64]\n"
     ]
    }
   ],
   "source": [
    "print(train_texts_vectorized[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d1e8b0-2979-40b8-8573-daa04690735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72161, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde3255-2bca-4bd1-9e8c-5530dd2bd185",
   "metadata": {},
   "source": [
    "### Save the text processor state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d32f4c-5ebc-486e-8d5d-5f247fa34c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processor_state.pkl', 'wb') as f:\n",
    " pickle.dump(processor, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c6d7b-2f9e-4b4c-80be-5b65be9cad87",
   "metadata": {},
   "source": [
    "### Create some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c36a542-41c8-4d9d-ab32-055533d37211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_1(vocab_size, embedding_dim, filters, kernel_size, dropout_rate, pool_size, classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=embedding_dim,\n",
    "                                        input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size,  activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(len(CLASSES), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63cb2d2-153e-48ac-a219-4d858c2e7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(vocab_size, embedding_dim, filters, kernel_size, dropout_rate, pool_size, classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                        output_dim=embedding_dim, \n",
    "                                        input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f31fc4c-3220-47b0-b7e0-f14cc51a05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(vocab_size, embedding_dim, filters, kernel_size, dropout_rate, pool_size, classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                        output_dim=embedding_dim, \n",
    "                                        input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab2dfc-6b3c-4003-8de8-2332c1bbfb33",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24a27f44-32ac-4f96-9f6e-b887d9cba691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 200)           4000000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 48, 64)            38464     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 14, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 4,063,555\n",
      "Trainable params: 4,063,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 10:04:47.661853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-28 10:04:47.661936: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-28 10:04:47.661969: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a58774662d82): /proc/driver/nvidia/version does not exist\n",
      "2022-04-28 10:04:47.662373: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = .001\n",
    "EMBEDDING_DIM = 200\n",
    "FILTERS = 64\n",
    "KERNEL_SIZE = 3\n",
    "DROPOUT_RATE = 0.2\n",
    "POOL_SIZE = 3\n",
    "CLASSES = ['github', 'nytimes', 'techcrunch']\n",
    "\n",
    "model = create_model_1(VOCAB_SIZE, EMBEDDING_DIM, FILTERS, KERNEL_SIZE, DROPOUT_RATE,POOL_SIZE, CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb41220-89f0-4f9e-b0f7-1d0d2b4ed906",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "774bc077-6c0d-4314-8ab1-b37a7f611b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72161, 3)\n",
      "(72161, 50)\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(train_texts_vectorized.shape)\n",
    "print(train_labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d787c37-e617-4787-ad82-4e10c49d0069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 10:04:47.852544: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564/564 [==============================] - 33s 57ms/step - loss: 0.5301 - acc: 0.7676\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.3932 - acc: 0.8423\n",
      "Eval loss/accuracy:[0.39317235350608826, 0.8423044681549072]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH=1\n",
    "BATCH_SIZE=128\n",
    "\n",
    "model.fit(train_texts_vectorized, train_labels, epochs=NUM_EPOCH, batch_size=BATCH_SIZE)\n",
    "print('Eval loss/accuracy:{}'.format(model.evaluate(eval_texts_vectorized, eval_labels, batch_size=BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb40fc5f-b5a2-434b-b280-3a76a71f2317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 10:05:22.521166: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
